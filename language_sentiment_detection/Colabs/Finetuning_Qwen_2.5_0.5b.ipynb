{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUAGFqQif69N"
      },
      "outputs": [],
      "source": [
        "# ==================== Installation ====================\n",
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install psutil\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install flash-attention==2.8.2\n",
        "!pip install scikit-learn  # For language-balanced sampling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Imports ====================\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from unsloth import FastModel\n",
        "from unsloth.trainer import SFTTrainer\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import TrainingArguments, TextStreamer"
      ],
      "metadata": {
        "id": "aXjj2BCAqDzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Configuration ====================\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Increased for better JSON generation\n",
        "MAX_SEQ_LENGTH = 192  # Increased from 128 to handle longer texts + JSON"
      ],
      "metadata": {
        "id": "r8Oma-cSqLtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 1. Load & Prepare Model ====================\n",
        "print(\"Loading model and tokenizer...\")\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-0.5B-Instruct\",  # Using Qwen 2.5 0.5B (slightly smaller than 0.6B but similar)\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=torch.bfloat16,\n",
        "    load_in_4bit=True,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        ")\n",
        "\n",
        "# Set a padding token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Apply PEFT with LoRA - UPDATED for better JSON generation\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    # Added more target modules for better instruction following\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "        \"lm_head\",  # Crucial for output format learning\n",
        "    ],\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=SEED,\n",
        ")"
      ],
      "metadata": {
        "id": "S8i0bIZPYoio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 2. Load & Prepare Dataset ====================\n",
        "print(\"Loading and formatting dataset...\")\n",
        "df = pd.read_csv(\"multilingual_sentiment.csv\")\n",
        "\n",
        "# Check dataset structure\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Languages: {df['lang'].unique()}\")\n",
        "print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
        "\n",
        "# ==================== CRITICAL: Language-Balanced Sampling ====================\n",
        "print(\"Applying language-balanced sampling...\")\n",
        "\n",
        "def balance_dataset_by_language(df, target_samples_per_lang=None):\n",
        "    \"\"\"\n",
        "    Balance dataset across languages to prevent bias\n",
        "    \"\"\"\n",
        "    lang_counts = df['lang'].value_counts()\n",
        "    print(f\"Original language distribution:\\n{lang_counts}\")\n",
        "\n",
        "    if target_samples_per_lang is None:\n",
        "        # Use the median count to avoid over/under sampling too much\n",
        "        target_samples_per_lang = int(lang_counts.median())\n",
        "\n",
        "    balanced_dfs = []\n",
        "    for lang in df['lang'].unique():\n",
        "        lang_df = df[df['lang'] == lang].copy()  # Make a copy to avoid warnings\n",
        "        if len(lang_df) > target_samples_per_lang:\n",
        "            # Under-sample if we have too many\n",
        "            lang_df = lang_df.sample(n=target_samples_per_lang, random_state=SEED, replace=False)\n",
        "        else:\n",
        "            # Over-sample if we have too few (with repetition)\n",
        "            lang_df = lang_df.sample(n=target_samples_per_lang, replace=True, random_state=SEED)\n",
        "        balanced_dfs.append(lang_df)\n",
        "\n",
        "    balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
        "    balanced_df = balanced_df.sample(frac=1, random_state=SEED)  # Shuffle\n",
        "\n",
        "    print(f\"Balanced language distribution:\\n{balanced_df['lang'].value_counts()}\")\n",
        "    return balanced_df\n",
        "\n",
        "# Balance the dataset\n",
        "df_balanced = balance_dataset_by_language(df)\n",
        "\n",
        "# ==================== IMPROVED: Format data for instruction tuning ====================\n",
        "def format_for_sft(row):\n",
        "    \"\"\"\n",
        "    Create instruction-tuning format with system message for JSON generation\n",
        "    \"\"\"\n",
        "    # Extract language from text if it contains <lang> tag, otherwise use lang column\n",
        "    text = row[\"text\"]\n",
        "    lang = row[\"lang\"]\n",
        "    sentiment = row[\"label\"]\n",
        "\n",
        "    # Create instruction with system message format\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a multilingual sentiment analyzer. Analyze the text and output valid JSON with language and sentiment tags. Only output the JSON, no additional text.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Text: {text}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f'{{\"language\": \"{lang}\", \"sentiment\": \"{sentiment}\"}}'}\n",
        "    ]\n",
        "\n",
        "    # Format for Qwen chat template\n",
        "    formatted = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "\n",
        "    return {\"text\": formatted}\n",
        "\n",
        "# Apply formatting\n",
        "print(\"Formatting data for instruction tuning...\")\n",
        "formatted_data = []\n",
        "for _, row in df_balanced.iterrows():\n",
        "    formatted_data.append(format_for_sft(row))\n",
        "\n",
        "formatted_df = pd.DataFrame(formatted_data)\n",
        "\n",
        "# Split dataset - preserve language distribution in splits\n",
        "print(\"Splitting dataset...\")\n",
        "def stratified_split_by_language(formatted_df, lang_column, test_size=0.15, val_size=0.15, random_state=42):\n",
        "    \"\"\"Split dataset while preserving language distribution in each split\"\"\"\n",
        "    # Add language column back for stratification\n",
        "    formatted_df = formatted_df.copy()\n",
        "    formatted_df['lang'] = df_balanced['lang'].values\n",
        "\n",
        "    # First split: train + temp\n",
        "    train_df, temp_df = train_test_split(\n",
        "        formatted_df,\n",
        "        test_size=test_size + val_size,\n",
        "        random_state=random_state,\n",
        "        stratify=formatted_df['lang']  # Stratify by language\n",
        "    )\n",
        "\n",
        "    # Second split: validation + test\n",
        "    val_ratio = val_size / (test_size + val_size)\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df,\n",
        "        test_size=1 - val_ratio,\n",
        "        random_state=random_state,\n",
        "        stratify=temp_df['lang']  # Stratify by language\n",
        "    )\n",
        "\n",
        "    # Remove the lang column before returning\n",
        "    train_df = train_df.drop(columns=['lang'])\n",
        "    val_df = val_df.drop(columns=['lang'])\n",
        "    test_df = test_df.drop(columns=['lang'])\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# Perform stratified split\n",
        "train_df, val_df, test_df = stratified_split_by_language(formatted_df, lang_column='lang')\n",
        "\n",
        "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
        "\n",
        "# Reset indices to avoid duplicate column issues\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "# Create DatasetDict\n",
        "print(\"Creating DatasetDict...\")\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': Dataset.from_dict(train_df.to_dict('list')),\n",
        "    'eval': Dataset.from_dict(val_df.to_dict('list')),\n",
        "    'test': Dataset.from_dict(test_df.to_dict('list')),\n",
        "})\n",
        "\n",
        "print(\"Dataset created successfully!\")"
      ],
      "metadata": {
        "id": "YmljRvH3QiPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 4. Configure Data Collator ====================\n",
        "# Response template for JSON generation start\n",
        "response_template = tokenizer('{\"language\":', add_special_tokens=False)['input_ids']\n",
        "print(f\"Response template tokens: {response_template}\")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "    pad_to_multiple_of=8,\n",
        ")"
      ],
      "metadata": {
        "id": "dyttE-rfqmO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 5. Training Arguments - OPTIMIZED for L4 ====================\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen_sentiment_finetuned\",\n",
        "    num_train_epochs=1,                # Increased for better convergence with small model\n",
        "    per_device_train_batch_size=192,     # Reduced for L4 memory (4-bit model + LoRA)\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=1,     # Effective batch size = 4 * 4 = 16\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    greater_is_better=False,\n",
        "    fp16=False,\n",
        "    bf16=True, # Changed from bf16 for broader compatibility\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=SEED,\n",
        "    report_to=\"none\",\n",
        "    dataloader_num_workers=2,\n",
        "    gradient_checkpointing=True,       # Enabled for L4 memory savings\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        ")"
      ],
      "metadata": {
        "id": "mLyCu1qLqrfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-tokenize everything before trainer initialization\n",
        "def pre_tokenize_dataset(dataset):\n",
        "    return dataset.map(\n",
        "        lambda x: tokenizer(\n",
        "            x[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=MAX_SEQ_LENGTH,\n",
        "            padding=\"max_length\"\n",
        "        ),\n",
        "        batched=True,\n",
        "        num_proc=1,  # Force single process\n",
        "        remove_columns=[\"text\"]\n",
        "    )\n",
        "\n",
        "tokenized_train = pre_tokenize_dataset(dataset_dict[\"train\"])\n",
        "tokenized_eval = pre_tokenize_dataset(dataset_dict[\"eval\"])\n",
        "\n",
        "# Then use these pre-tokenized datasets\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,    # Already tokenized\n",
        "    data_collator=data_collator,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    # Don't use dataset_text_field since data is already tokenized\n",
        "    dataset_text_field=None,\n",
        "    packing=False,\n",
        ")"
      ],
      "metadata": {
        "id": "yy4u2vYfcqix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 7. Train & Save ====================\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"Training complete. Saving model...\")\n",
        "model.save_pretrained(\"./final_sentiment_model\")\n",
        "tokenizer.save_pretrained(\"./final_sentiment_model\")\n",
        "print(\"Model saved.\")\n"
      ],
      "metadata": {
        "id": "ge3F-ngxrC16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Load model\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name=\"./final_sentiment_model\",\n",
        "    max_seq_length=192,\n",
        "    load_in_4bit=False,\n",
        ")\n",
        "\n",
        "def test_model(text):\n",
        "    \"\"\"One-line test function\"\"\"\n",
        "    prompt = f\"\"\"<|system|>\n",
        "You are a multilingual sentiment analyzer. Output JSON with language and sentiment.\n",
        "\n",
        "<|user|>\n",
        "Text: {text}\n",
        "\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(response)\n",
        "\n",
        "# Quick test\n",
        "print(test_model(\"I'm very happy with this!\"))\n",
        "print(test_model(\"No me gusta nada esto.\"))"
      ],
      "metadata": {
        "id": "vEaAjBiOUuZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Models in different precision"
      ],
      "metadata": {
        "id": "YZYNTo2k54cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Paths\n",
        "lora_path = \"/content/final_sentiment_model\"\n",
        "base_model_name = \"unsloth/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "# ==================== 1. FP16 (Default Precision) ====================\n",
        "print(\"Saving in FP16...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    tie_word_embeddings=False,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "peft_model = PeftModel.from_pretrained(base_model, lora_path)\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"/content/qwen_langsenti_merged_fp16\")\n",
        "\n",
        "# ==================== 2. INT8 (8-bit Quantization) ====================\n",
        "print(\"Saving in INT8...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_quant_type=\"qat8\",\n",
        "    bnb_8bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "int8_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "peft_model_int8 = PeftModel.from_pretrained(int8_model, lora_path)\n",
        "merged_int8 = peft_model_int8.merge_and_unload()\n",
        "merged_int8.save_pretrained(\"/content/qwen_langsenti_merged_int8\")\n",
        "\n",
        "# ==================== 3. INT4 (4-bit Quantization) ====================\n",
        "print(\"Saving in INT4...\")\n",
        "bnb_config_4bit = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "int4_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config_4bit,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "peft_model_int4 = PeftModel.from_pretrained(int4_model, lora_path)\n",
        "merged_int4 = peft_model_int4.merge_and_unload()\n",
        "merged_int4.save_pretrained(\"/content/qwen_langsenti_merged_int4\")\n",
        "\n",
        "# ==================== Save Tokenizer ====================\n",
        "tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
        "tokenizer.save_pretrained(\"/content/qwen_langsenti_merged_fp16\")\n",
        "tokenizer.save_pretrained(\"/content/qwen_langsenti_merged_int8\")\n",
        "tokenizer.save_pretrained(\"/content/qwen_langsenti_merged_int4\")\n",
        "\n",
        "print(\"All precision models saved!\")"
      ],
      "metadata": {
        "id": "PQ7rMixH564w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r qwen_langsenti_merged_int8.zip qwen_langsenti_merged_int8\n",
        "!zip -r qwen_langsenti_merged_int4.zip qwen_langsenti_merged_int4"
      ],
      "metadata": {
        "id": "5DU1YOXzSDRS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.10"
      },
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}