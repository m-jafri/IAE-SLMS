{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE8SgOwAjfPM"
      },
      "source": [
        "#Install and setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iqROdNmjkdS"
      },
      "source": [
        "##1. Install Unsloth & Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg6xZdI4jbpQ"
      },
      "outputs": [],
      "source": [
        "# @title 1. Install Unsloth & Load Model\n",
        "# Installs unsloth and loads Llama-3-8B-Instruct (4-bit) specifically optimized for Colab\n",
        "\n",
        "# 1. Install Unsloth (specifically for Colab)\n",
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "\n",
        "# 2. Import & Load Model\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048 # Roman Urdu is token-heavy, but 2048 is safe for Colab\n",
        "dtype = None # Auto detection\n",
        "load_in_4bit = True # Essential for Colab T4 GPU\n",
        "\n",
        "# We use Llama 3.1 8B Instruct (Best balance of size/performance)\n",
        "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
        "\n",
        "print(f\"ðŸ”„ Loading {model_name}...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# 3. Add LoRA Adapters (The Fine-Tuning Layer)\n",
        "# We target \"all-linear\" modules to help it learn the new language patterns better\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Rank: 16 is standard. 32 or 64 is better for new languages but uses more VRAM.\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Optimized to 0 for Unsloth\n",
        "    bias = \"none\",    # Optimized to \"none\" for Unsloth\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "print(\"âœ… Model Loaded & LoRA Adapters Attached!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV9jfJlWjzcZ"
      },
      "source": [
        "#Dataformatting and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3oYcbDQj0Zm"
      },
      "source": [
        "##2. Format Data & Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_thIJBXjoiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c2c688c-684d-466b-9447-9c7df9b7d137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUp7eBIHj57h"
      },
      "outputs": [],
      "source": [
        "# @title 2. Format Data & Start Training\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Load your Dataset again\n",
        "dataset = load_dataset(\"Redgerd/roman-urdu-alpaca-qa-mix\", split = \"train\")\n",
        "\n",
        "# 2. Define the Chat Format (Alpaca -> Llama 3 Prompt)\n",
        "# This teaches the model: \"Here is an instruction, here is the context, give me the answer.\"\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Fill the template\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# Apply formatting\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
        "\n",
        "# 3. Set up the Trainer (Updated for 500 Steps + Google Drive Save)\n",
        "print(\"ðŸš€ Starting Trainer Setup...\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "\n",
        "        # --- KEY CHANGES HERE ---\n",
        "        max_steps = 500,        # Increased to 500 for better quality\n",
        "        save_steps = 100,       # Save a backup every 100 steps\n",
        "        output_dir = \"/content/drive/My Drive/RomanUrduModel\", # Save directly to Drive\n",
        "        # ------------------------\n",
        "\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 4. Train!\n",
        "print(\"ðŸ”¥ Training Started!\")\n",
        "# trainer_stats = trainer.train()\n",
        "# New line to resume from your specific checkpoint\n",
        "print(\"New line to resume from my specific checkpoint\\n\")\n",
        "trainer.train(resume_from_checkpoint=\"/content/drive/MyDrive/RomanUrduModel/checkpoint-500\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the model is saved on drive"
      ],
      "metadata": {
        "id": "RIZIaU71UpeQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CuSR5hakCfH"
      },
      "source": [
        "#Testing after training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inferences and testing is done in seperate colab file : https://colab.research.google.com/drive/1ob1OQgX7OoTNNAUGn1nsfywOx8y7Yc7h?usp=sharing"
      ],
      "metadata": {
        "id": "t7Bl4u3KTyYQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9Xv3-54kI8a"
      },
      "source": [
        "##4. Launch Public Web App (Gradio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh2hW-qEkABT"
      },
      "outputs": [],
      "source": [
        "# # @title 4. Launch Public Web App (Gradio)\n",
        "# # This creates a shareable link to your model\n",
        "# !pip install -q gradio\n",
        "\n",
        "# import gradio as gr\n",
        "# from transformers import TextStreamer\n",
        "\n",
        "# # 1. Enable Native Fast Inference\n",
        "# FastLanguageModel.for_inference(model)\n",
        "\n",
        "# def chat_response(message, history):\n",
        "#     # 'message' is the user's new question\n",
        "#     # 'history' is the chat context (we are ignoring it for simple QA to save memory,\n",
        "#     # but you can add it if you want multi-turn memory)\n",
        "\n",
        "#     # Prepare the prompt\n",
        "#     prompt = alpaca_prompt.format(\n",
        "#         message, # instruction\n",
        "#         \"\",      # input (optional context)\n",
        "#         \"\",      # output - leave blank\n",
        "#     )\n",
        "\n",
        "#     # Tokenize and move to GPU\n",
        "#     inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "#     # Generate\n",
        "#     outputs = model.generate(\n",
        "#         **inputs,\n",
        "#         max_new_tokens = 256,\n",
        "#         use_cache = True\n",
        "#     )\n",
        "\n",
        "#     # Decode and clean up (remove the prompt from the answer)\n",
        "#     decoded_output = tokenizer.batch_decode(outputs)[0]\n",
        "#     cleaned_response = decoded_output.split(\"### Response:\")[-1].strip().replace(\"<|end_of_text|>\", \"\")\n",
        "\n",
        "#     return cleaned_response\n",
        "\n",
        "# # 2. Build the Interface\n",
        "# demo = gr.ChatInterface(\n",
        "#     fn=chat_response,\n",
        "#     title=\"ðŸ¤– My Roman Urdu AI\",\n",
        "#     description=\"Ask me anything in Roman Urdu! (e.g., 'Biryani kaise banate hain?')\",\n",
        "#     examples=[\"Pakistan ka capital kya hai?\", \"Biryani ki recipe batao\", \"Lahore kyun mashhoor hai?\"],\n",
        "#     theme=\"soft\"\n",
        "# )\n",
        "\n",
        "# # 3. Launch with Public Link\n",
        "# # share=True creates the public URL\n",
        "# print(\"ðŸš€ Launching... Click the link that ends in '.gradio.live' below!\")\n",
        "# demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BUZUaIjlycn"
      },
      "source": [
        "#Save to GGUF (16-bit)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code is continue in the same testing/inferences colab file : https://colab.research.google.com/drive/1ob1OQgX7OoTNNAUGn1nsfywOx8y7Yc7h?usp=sharing"
      ],
      "metadata": {
        "id": "gqoxSMfNUV20"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw2bRSXvkLZ-"
      },
      "outputs": [],
      "source": [
        "# # @title 5. Save Model to GGUF (For Deployment)\n",
        "# # Ye code aapke model ko ek single file (model.gguf) mein convert karke\n",
        "# # Google Drive mein save kar dega.\n",
        "\n",
        "# # 1. Save to GGUF (16-bit)\n",
        "# # Ye file thori bari hogi lekin quality best hogi.\n",
        "# if False: model.save_pretrained_gguf(\"model_f16\", tokenizer, quantization_method = \"f16\")\n",
        "\n",
        "# # 2. Save to GGUF (8-bit Quantized) - RECOMMENDED\n",
        "# # Ye file size mein choti hogi aur fast chalegi.\n",
        "# print(\"ðŸ’¾ Saving model to Google Drive as GGUF...\")\n",
        "# model.save_pretrained_gguf(\n",
        "#     \"/content/drive/My Drive/RomanUrduModel/model_q8\", # Folder in Drive\n",
        "#     tokenizer,\n",
        "#     quantization_method = \"q8_0\" # 8-bit quantization\n",
        "# )\n",
        "# print(\"âœ… DONE! Check your Google Drive folder for 'model_q8.gguf'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Move code from drive to hugging face"
      ],
      "metadata": {
        "id": "QpCVTJX2u-k5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this code is on testing/inferences colab file : https://colab.research.google.com/drive/1ob1OQgX7OoTNNAUGn1nsfywOx8y7Yc7h?usp=sharing"
      ],
      "metadata": {
        "id": "IN4YYvu7UdXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title 6. Push Model to Hugging Face Hub\n",
        "# from huggingface_hub import login\n",
        "\n",
        "# # 1. Login (Paste your token when asked)\n",
        "# # Yahan apna \"Write\" token paste karein\n",
        "# hf_token = \"I will add it here\" # <--- Paste Token Here\n",
        "# login(token=hf_token)\n",
        "\n",
        "# # 2. Upload to Hugging Face\n",
        "# username = \"Aapka_HF_Username\" # <--- Apna Hugging Face username likhein\n",
        "# model_name = \"Roman-Urdu-Llama-3\"     # Model ka naam jo rakhna hai\n",
        "\n",
        "# print(\"â˜ï¸ Uploading model to Hugging Face...\")\n",
        "# # Ye GGUF (8-bit) wala upload karega jo sabse acha hai\n",
        "# model.push_to_hub_gguf(\n",
        "#     f\"{username}/{model_name}\",\n",
        "#     tokenizer,\n",
        "#     quantization_method = \"q8_0\",\n",
        "#     token = hf_token\n",
        "# )\n",
        "\n",
        "# print(f\"âœ… DONE! Model is live at: https://huggingface.co/{username}/{model_name}\")"
      ],
      "metadata": {
        "id": "uJPJ4-7GvBqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nDHdMXYm6MZ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}